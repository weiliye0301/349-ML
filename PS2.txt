HW2Liye WeiNetID: lwq3505Q1: The attribute of alcohol seems to be the most useful attribute for classifying the wine. Because the most positive values have least negative values while the most negative values have least positive values, and that means most data correspond to this standard.Q2: The accuracy is 62.381% achieved by ZeroR when I run it on the training set. Since ZeroR is the simplest classification method that relies on the target and ignores all predictors and simply predicts the majority category, thus it is useful for determining a baseline performance as a benchmark for other classification methods.Q3: The alcohol is the most informative single feature for this task. Within the decision tree, the alcohol is mainly divided into two parts to discuss by 10.8 and all the quality is good over 12. And this matches my answer from question1.Q4: In 10-fold cross-validation, the original sample is randomly partitioned into 10 equal sized subsamples. The main difference is that of the 10 subsamples, a single subsample is retained as the validation data for testing the model, and the remaining 9 subsamples are used as training data. The cross-validation process is then repeated 10 times, with each of the 10 subsamples used exactly once as the validation data. The 10 results from the folds can then be averaged to produce a single estimation. The importance of this method is that all observations are used for both training and validation, and each observation is used for validation exactly once.Q5: The “command-line” for the model is RandomTree -K 5 -M 1.0 -V 0.001 -S 1. And the reported accuracy for my model using 10-fold cross-validation is 88.0423%.Q6: First of all, I run all the models and noted their accuracies. Among those models hold 100% accuracy, I run them again with 10-cross-validation set, and then RandomTree -K 1 -M 1.0 -V 0.001 -S 1 was chosen for high accuracy with low run time. And then I modifies several parameter include KValue, minNum, minVarianceProp and seed, KValue could provide better accuracy for the value 5 rather than 1. Therefore, the final model RandomTree -K 5 -M 1.0 -V 0.001 -S 1 is selected and modified.Q7: The strategy I applied was find the classifier that fit the car data best and converted to wine data. In the meanwhile, the classifier I chose for the wine data was also applied on the car data for estimation.The classifier A I choose is RandomTree -K 5 -M 1.0 -V 0.001 -S 1 and the classifier B I choose is MultilayerPerceptron -L 0.3 -M 0.2 -N 500 -V 0 -S 0 -E 20 -H a.wine_acc(A) is 88.0423%.car_acc(B) is 99.2437 %.wine_acc(B) is 84.9735 %car_acc(A) is 90.3361 %wine_acc(A) + car_acc(B) – wine_acc(B) – car_acc(A)=11.9764%Q8: f1 is one-nearest-neighbor. The value is simple assigned to the class of single nearest neighbor.f2 is linear regression. The dataset is similar to a linear fit compared x and f2(x) values.f3 is polynomial regression. The dataset is similar to a two-square polynomial fit compared x and f3(x) values.f4 is three-nearest-neighbor. The key element is when x=4, f4(x) values corresponds to x=1 as well as x=7 values.